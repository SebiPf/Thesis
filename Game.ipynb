{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import mbcs_decode\n",
    "from ctypes.wintypes import WORD\n",
    "from pickle import TUPLE\n",
    "from platform import python_branch\n",
    "import gym\n",
    "from gym import Env\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gym import spaces\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 3}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=7):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        self.human = 'human'\n",
    "        self.rgb = 'rgb_array'\n",
    "        \n",
    "        self.observation_space = spaces.Box(np.array([0,0]), np.array([size-1,size-1]), shape=(2,),dtype=int)\n",
    "        print(self.observation_space)\n",
    "        \n",
    "        #Moveset of the Agent (Left, Right, Up, Down, Stay)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0],dtype=int),\n",
    "            1: np.array([0, 1],dtype=int),\n",
    "            2: np.array([-1, 0],dtype=int),\n",
    "            3: np.array([0, -1],dtype=int),\n",
    "            #4: np.array([0,0])\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "\n",
    "    def _get_Agent(self):\n",
    "        # if(self._target_location[0] and self._target_location[1]  in np.array( [self._agent_location + [1,1],\n",
    "        #                                         self._agent_location +  [1,-1],\n",
    "        #                                         self._agent_location +  [-1,-1],\n",
    "        #                                         self._agent_location +  [-1,1],\n",
    "        #                                         self._agent_location +  [2,0],\n",
    "        #                                         self._agent_location +  [0,2],\n",
    "        #                                         self._agent_location +  [-2,0],\n",
    "        #                                         self._agent_location +  [0,-2],\n",
    "        #                                         self._agent_location +  [1,0],\n",
    "        #                                         self._agent_location +  [-1,0],\n",
    "        #                                         self._agent_location +  [0,1],\n",
    "        #                                         self._agent_location +  [0,-1]], dtype=int)):\n",
    "            #pygame.quit()\n",
    "        # distance = np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "        # if distance <= 2:\n",
    "        #     #return {\"target\":self._target_location,\"agent\": self._agent_location }\n",
    "        #     return self._target_location,self._agent_location\n",
    "\n",
    "        # else:\n",
    "        #     #return {\"agent\": self._agent_location}\n",
    "        #     return self._agent_location\n",
    "        return self._agent_location\n",
    "    def _get_Target(self):\n",
    "        #return {\"target\":self._target_location}\n",
    "        return self._target_location\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, seed=2, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location= self.np_random.integers(\n",
    "                 self.size-1, self.size, size=2, dtype=int\n",
    "            ) \n",
    "        \n",
    "        self._obstacle_location = np.array([2,2])\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location) or np.array_equal(self._target_location, self._obstacle_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                 0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            self._render_frame()\n",
    "        \n",
    "        self.observation = self._get_Agent()\n",
    "        \n",
    "        self.info = self._get_Target()\n",
    "        return self.observation, self.info\n",
    "\n",
    "    def step(self, action):\n",
    "            # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        self.reward_gain = 1\n",
    "        self.reward_loss = 0.01\n",
    "        self.steps += 1\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        if(self._agent_location + direction ==self._obstacle_location).all():\n",
    "            self._agent_location = self._agent_location -direction\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1,\n",
    "            \n",
    "        )\n",
    "        # An episode is done if the agent has reached the target\n",
    "        self.terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        self.reward += self.reward_gain if self.terminated else - self.reward_loss  # Binary sparse rewards\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            self._render_frame()\n",
    "        self.observation = self._get_Agent()\n",
    "        self.info = self._get_Target()\n",
    "        return (self.observation, self.reward, self.terminated, False, self.steps, self.info)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == self.rgb:\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == self.human:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == self.human:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._obstacle_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 6, (2,), int32)\n"
     ]
    }
   ],
   "source": [
    "env = Game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 6, (2,), int32)\n"
     ]
    }
   ],
   "source": [
    "human = 'human'\n",
    "rgb = 'rgb_array'\n",
    "env = Game(render_mode=rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m      2\u001b[0m episodes \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m      3\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[28], line 88\u001b[0m, in \u001b[0;36mGame.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuman:\n\u001b[0;32m     86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_frame()\n\u001b[1;32m---> 88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_Agent()\n\u001b[0;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_Target()\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\n",
      "Cell \u001b[1;32mIn[28], line 59\u001b[0m, in \u001b[0;36mGame._get_Agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_Agent\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     38\u001b[0m     \u001b[39m# if(self._target_location[0] and self._target_location[1]  in np.array( [self._agent_location + [1,1],\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[39m#                                         self._agent_location +  [1,-1],\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39m#     #return {\"agent\": self._agent_location}\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[39m#     return self._agent_location\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_location)\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\numpy\\__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[1;32m--> 311\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "episodes = 20\n",
    "score = 0\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation = env.step(action)\n",
    "        env.render()\n",
    "        steps +=1\n",
    "        if(np.array_equal(env._agent_location, env._target_location)):\n",
    "            score += env.reward\n",
    "            done = True\n",
    "            \n",
    "    print('Episode:{} Score:{}'.format(episode,score), \"steps:{} \".format(steps))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(actions)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(layers.Dense(24, activation='relu', input_shape=(1,2)))\n",
    "    #model.add(layers.Dense(24, activation='relu', input_shape=(1,4)))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(actions, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1, 24)             72        \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 24)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 772\n",
      "Trainable params: 772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_3_input to have 3 dimensions, but got array with shape (1, 1, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(model, actions)\n\u001b[0;32m      2\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m dqn\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\rl\\core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    165\u001b[0m callbacks\u001b[39m.\u001b[39mon_step_begin(episode_step)\n\u001b[0;32m    166\u001b[0m \u001b[39m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(observation)\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_q_values(state)\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[0;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:1303\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1299\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`predict_on_batch` is not supported for models distributed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith tf.distribute.Strategy.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m     )\n\u001b[0;32m   1302\u001b[0m \u001b[39m# Validate and standardize user data.\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m inputs, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m   1304\u001b[0m     x, extract_tensors_from_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m   1305\u001b[0m )\n\u001b[0;32m   1306\u001b[0m \u001b[39m# If `self._distribution_strategy` is True, then we are in a replica\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39m# context at this point.\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_eagerly \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2650\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2642\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2643\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2646\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2647\u001b[0m ):\n\u001b[0;32m   2648\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2650\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2651\u001b[0m     x,\n\u001b[0;32m   2652\u001b[0m     y,\n\u001b[0;32m   2653\u001b[0m     sample_weight,\n\u001b[0;32m   2654\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2655\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2656\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2657\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2658\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2659\u001b[0m )\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2691\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2690\u001b[0m     \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2691\u001b[0m     x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2692\u001b[0m         x,\n\u001b[0;32m   2693\u001b[0m         feed_input_names,\n\u001b[0;32m   2694\u001b[0m         feed_input_shapes,\n\u001b[0;32m   2695\u001b[0m         check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2696\u001b[0m         exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2697\u001b[0m     )\n\u001b[0;32m   2699\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39m# standardization code.\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32mc:\\Development\\Theses\\Thesis\\.venv\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:712\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    710\u001b[0m shape \u001b[39m=\u001b[39m shapes[i]\n\u001b[0;32m    711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_shape) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(shape):\n\u001b[1;32m--> 712\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    713\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m         \u001b[39m+\u001b[39m exception_prefix\n\u001b[0;32m    715\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[39m+\u001b[39m names[i]\n\u001b[0;32m    717\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    718\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(shape))\n\u001b[0;32m    719\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m dimensions, but got array with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(data_shape)\n\u001b[0;32m    721\u001b[0m     )\n\u001b[0;32m    722\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_batch_axis:\n\u001b[0;32m    723\u001b[0m     data_shape \u001b[39m=\u001b[39m data_shape[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_3_input to have 3 dimensions, but got array with shape (1, 1, 2, 2)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=500, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "903da58cf30ebd1d6c25238449ec7b9b792b651be3cbe77229884cf5a080d95f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
