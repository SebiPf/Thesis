{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import mbcs_decode\n",
    "from ctypes.wintypes import WORD\n",
    "from pickle import TUPLE\n",
    "from platform import python_branch\n",
    "import gym\n",
    "from gym import Env\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gym import spaces\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 3}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=7):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        self.human = 'human'\n",
    "        self.rgb = 'rgb_array'\n",
    "        \n",
    "        self.observation_space = spaces.Box(np.array([0,0]), np.array([size-1,size-1]), shape=(2,),dtype=int)\n",
    "        print(self.observation_space)\n",
    "        \n",
    "        #Moveset of the Agent (Left, Right, Up, Down, Stay)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0],dtype=int),\n",
    "            1: np.array([0, 1],dtype=int),\n",
    "            2: np.array([-1, 0],dtype=int),\n",
    "            3: np.array([0, -1],dtype=int),\n",
    "            #4: np.array([0,0])\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "\n",
    "    def _get_Agent(self):\n",
    "        # if(self._target_location[0] and self._target_location[1]  in np.array( [self._agent_location + [1,1],\n",
    "        #                                         self._agent_location +  [1,-1],\n",
    "        #                                         self._agent_location +  [-1,-1],\n",
    "        #                                         self._agent_location +  [-1,1],\n",
    "        #                                         self._agent_location +  [2,0],\n",
    "        #                                         self._agent_location +  [0,2],\n",
    "        #                                         self._agent_location +  [-2,0],\n",
    "        #                                         self._agent_location +  [0,-2],\n",
    "        #                                         self._agent_location +  [1,0],\n",
    "        #                                         self._agent_location +  [-1,0],\n",
    "        #                                         self._agent_location +  [0,1],\n",
    "        #                                         self._agent_location +  [0,-1]], dtype=int)):\n",
    "            #pygame.quit()\n",
    "        # distance = np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "        # if distance <= 2:\n",
    "        #     #return {\"target\":self._target_location,\"agent\": self._agent_location }\n",
    "        #     return self._target_location,self._agent_location\n",
    "\n",
    "        # else:\n",
    "        #     #return {\"agent\": self._agent_location}\n",
    "        #     return self._agent_location\n",
    "        return self._agent_location\n",
    "    def _get_Target(self):\n",
    "        #return {\"target\":self._target_location}\n",
    "        return self._target_location\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        #super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location= np.random.randint(\n",
    "                 self.size-1, self.size, size=2\n",
    "            ) \n",
    "        \n",
    "        self._obstacle_location = np.array([2,2])\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location) or np.array_equal(self._target_location, self._obstacle_location):\n",
    "            self._target_location = np.random.randint(\n",
    "                 0, self.size, size=2\n",
    "            )\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            self._render_frame()\n",
    "        \n",
    "        self.observation = self._get_Agent()\n",
    "        \n",
    "        self.info = {}\n",
    "        return self.observation\n",
    "\n",
    "    def step(self, action):\n",
    "            # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        self.reward_gain = 1\n",
    "        self.reward_loss = 0.01\n",
    "        self.steps += 1\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        if(self._agent_location + direction ==self._obstacle_location).all():\n",
    "            self._agent_location = self._agent_location -direction\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1,\n",
    "            \n",
    "        )\n",
    "        # An episode is done if the agent has reached the target\n",
    "        self.terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        self.reward += self.reward_gain if self.terminated else - self.reward_loss  # Binary sparse rewards\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            self._render_frame()\n",
    "        self.observation = self._get_Agent()\n",
    "        self.info = {}\n",
    "        return (self.observation, self.reward,self.terminated, self.info)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == self.rgb:\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == self.human:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == self.human:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._obstacle_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == self.human:\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([0 0], [6 6], (2,), int32)\n"
     ]
    }
   ],
   "source": [
    "env = Game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([0 0], [6 6], (2,), int32)\n"
     ]
    }
   ],
   "source": [
    "human = 'human'\n",
    "rgb = 'rgb_array'\n",
    "env = Game(render_mode=rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:0.88 steps:13 \n",
      "Episode:2 Score:0.6699999999999999 steps:34 \n",
      "Episode:3 Score:0.48999999999999977 steps:52 \n",
      "Episode:4 Score:0.6699999999999999 steps:34 \n",
      "Episode:5 Score:-0.9200000000000015 steps:193 \n",
      "Episode:6 Score:1 steps:1 \n",
      "Episode:7 Score:-1.3299999999999943 steps:234 \n",
      "Episode:8 Score:-2.2199999999999753 steps:323 \n",
      "Episode:9 Score:0.91 steps:10 \n",
      "Episode:10 Score:-0.2800000000000009 steps:129 \n",
      "Episode:11 Score:-1.99999999999998 steps:301 \n",
      "Episode:12 Score:-1.0699999999999998 steps:208 \n",
      "Episode:13 Score:0.1799999999999995 steps:83 \n",
      "Episode:14 Score:-1.0699999999999998 steps:208 \n",
      "Episode:15 Score:-2.0699999999999785 steps:308 \n",
      "Episode:16 Score:0.14999999999999947 steps:86 \n",
      "Episode:17 Score:0.48999999999999977 steps:52 \n",
      "Episode:18 Score:-0.33000000000000096 steps:134 \n",
      "Episode:19 Score:-1.6999999999999864 steps:271 \n",
      "Episode:20 Score:0.1799999999999995 steps:83 \n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "episodes = 20\n",
    "score = 0\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, info = env.step(action)\n",
    "        env.render()\n",
    "        steps +=1\n",
    "        if(np.array_equal(env._agent_location, env._target_location)):\n",
    "            score += env.reward\n",
    "            done = True\n",
    "            \n",
    "    print('Episode:{} Score:{}'.format(episode,score), \"steps:{} \".format(steps))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(actions)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()  \n",
    "    model.add(layers.Dense(24, activation='relu', input_shape=(1,2)))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(actions, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_82 (Dense)            (None, 1, 24)             72        \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 24)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 772\n",
      "Trainable params: 772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -4.4080\n",
      "34 episodes - episode_reward: -1295.973 [-17436.770, 0.950] - loss: 8.347 - mae: 33.202 - mean_q: -43.141\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -7.9594\n",
      "26 episodes - episode_reward: -3061.854 [-25120.600, 1.000] - loss: 75.677 - mae: 146.053 - mean_q: -191.694\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -4.6274\n",
      "29 episodes - episode_reward: -1382.355 [-19551.520, 0.950] - loss: 148.297 - mae: 221.000 - mean_q: -291.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -8.3454\n",
      "45 episodes - episode_reward: -1991.296 [-75523.400, 1.000] - loss: 233.469 - mae: 268.608 - mean_q: -354.320\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -11.6086\n",
      "done, took 368.223 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a196d72530>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "851391c1fb3ab3d4ed73bf12ec5547297a7e91120229734f9607c1e545c47f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
