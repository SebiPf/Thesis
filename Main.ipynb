{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import mbcs_decode\n",
    "from ctypes.wintypes import WORD\n",
    "from pickle import TUPLE\n",
    "from platform import python_branch\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gym import spaces\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "#from keras import rl\n",
    "#import tesnsorflow as tf\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "#from tensorflow.keras.layers import Dense, Flatten\n",
    "#import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 3}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=7):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space1 = spaces.Box(0,size-1, shape=(2,),dtype=int)\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"obstacle\": spaces.Box(0,size-1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\",stay\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0],dtype=int),\n",
    "            1: np.array([0, 1],dtype=int),\n",
    "            2: np.array([-1, 0],dtype=int),\n",
    "            3: np.array([0, -1],dtype=int),\n",
    "            #4: np.array([0,0])\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "   \n",
    "\n",
    "    #def _get_obs(self):\n",
    "     #   return {\"agent\": self._agent_location, \"target\": self._target_location,  \"Obstacle\": self._obstacle_location}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        if(self._target_location[0] and self._target_location[1]  in np.array( [self._agent_location + [1,1],\n",
    "                                                self._agent_location +  [1,-1],\n",
    "                                                self._agent_location +  [-1,-1],\n",
    "                                                self._agent_location +  [-1,1],\n",
    "                                                self._agent_location +  [2,0],\n",
    "                                                self._agent_location +  [0,2],\n",
    "                                                self._agent_location +  [-2,0],\n",
    "                                                self._agent_location +  [0,-2],\n",
    "                                                self._agent_location +  [1,0],\n",
    "                                                self._agent_location +  [-1,0],\n",
    "                                                self._agent_location +  [0,1],\n",
    "                                                self._agent_location +  [0,-1]], dtype=int)):\n",
    "            #pygame.quit()\n",
    "            \n",
    "            return {\"target\":self._target_location,\"agent\": self._agent_location }\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            return {\"agent\": self._agent_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "\n",
    "    def _get_steps(self):\n",
    "        return {\"steps\":self.steps}\n",
    "    #def _get_obstacle(self):\n",
    "    #    return {\"distance\": np.linalg.norm(self._agent_location - self._obstacle_location, ord=1)}\n",
    "\n",
    "    def reset(self, seed=2, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        #self._agent_location = int(8,8)\n",
    "        self._agent_location= self.np_random.integers(\n",
    "                 self.size-1, self.size, size=2, dtype=int\n",
    "            ) \n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "        self._obstacle_location = np.array([2,2])\n",
    "        \n",
    " \n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location) or np.array_equal(self._target_location, self._obstacle_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                 0, self.size, size=2, dtype=int\n",
    "            )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "            self._obstacle_location = self._target_location\n",
    "            while np.array_equal(self._target_location, self._obstacle_location):\n",
    "                self._obstacle_location = self.np_random.integers(\n",
    "                    0, self.size, size=2, dtype=int\n",
    "                )\n",
    "        \"\"\"\n",
    "        self.observation = self._get_obs()\n",
    "        self.info = self._get_info()\n",
    "        self.observation_steps = self._get_steps()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return self.observation, self.info, self.observation_steps\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        self.steps += 1\n",
    "        direction = self._action_to_direction[action]\n",
    "        #while(np.array_equal(newloc, self._obstacle_location)):\n",
    "        #    direction = self._action_to_direction[action]\n",
    "        #    newloc = np.clip(\n",
    "        #    self._agent_location + direction, 0, self.size - 1\n",
    "        #    )\n",
    "        if(self._agent_location + direction ==self._obstacle_location).all():\n",
    "            self._agent_location = self._agent_location -direction\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        #self._agent_location = np.clip(\n",
    "        #self._agent_location + direction,self._obstacle_location\n",
    "        #)\n",
    "\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        self.terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "\n",
    "        \n",
    "        self.reward += 1 if self.terminated else -0.01  # Binary sparse rewards\n",
    "        self.observation = self._get_obs()\n",
    "        self.info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return (self.observation, self.reward, self.terminated, False, self.info, self.steps)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._obstacle_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridWorldEnv(render_mode=\"rgb_array\")\n",
    "env.observation_space1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "Episode:1 Score:0.91 steps:10 \n",
      "Episode:2 Score:-1.06 steps:207 \n",
      "Episode:3 Score:0.34999999999999964 steps:66 \n",
      "Episode:4 Score:0.1899999999999995 steps:82 \n",
      "Episode:5 Score:0.75 steps:26 \n",
      "Episode:6 Score:-3.759999999999943 steps:477 \n",
      "Episode:7 Score:0.83 steps:18 \n",
      "Episode:8 Score:-0.0400000000000007 steps:105 \n",
      "Episode:9 Score:0.72 steps:29 \n",
      "Episode:10 Score:-1.52999999999999 steps:254 \n",
      "Episode:11 Score:0.37999999999999967 steps:63 \n",
      "Episode:12 Score:-3.149999999999956 steps:416 \n",
      "Episode:13 Score:0.7699999999999999 steps:24 \n",
      "Episode:14 Score:0.5199999999999998 steps:49 \n",
      "Episode:15 Score:-0.23000000000000087 steps:124 \n",
      "Episode:16 Score:0.3999999999999997 steps:61 \n",
      "Episode:17 Score:-1.8699999999999828 steps:288 \n",
      "Episode:18 Score:0.6199999999999999 steps:39 \n",
      "Episode:19 Score:-0.07000000000000073 steps:108 \n",
      "Episode:20 Score:-0.020000000000000684 steps:103 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#env = GridWorldEnv(render_mode=\"rgb_array\")\n",
    "#print (env.observation_space1.sample,\"aaaaaaaaaaa\")\n",
    "env.observation_space1.sample()\n",
    "print (\"aa\")\n",
    "env.reset()\n",
    "steps = 0\n",
    "episodes = 20\n",
    "for episode in range(1, episodes+1):\n",
    "    env.observation = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    #print(env.observation)\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        env.observation = env.step(action)\n",
    "        env.render()\n",
    "        steps +=1\n",
    "        #print(env._agent_location)\n",
    "        if(np.array_equal(env._agent_location, env._target_location)):\n",
    "            done = True\n",
    "            \n",
    "            score += env.reward\n",
    "            \n",
    "    print('Episode:{} Score:{}'.format(episode,score), \"steps:{} \".format(steps))\n",
    "\n",
    "#plt.plot(episode,steps)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "states  = env.observation_space1.shape\n",
    "actions = env.action_space.n\n",
    "print(states)\n",
    "states = np.reshape(states,(states[0]-1))\n",
    "print(states)\n",
    "def build_model(states, actions):\n",
    "        model = Sequential() \n",
    "         \n",
    "        \n",
    "        model.add(layers.Dense(24, activation='relu', input_shape=(1,3)))\n",
    "        \n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dense(actions, activation='linear'))\n",
    "        model.add(layers.Flatten()) \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = build_model(states, actions)\n",
    "#model = np.reshape(model,model[2,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 1, 24)             96        \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1, 24)             600       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1, 4)              100       \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796\n",
      "Trainable params: 796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model,actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [95], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(model, actions)\n\u001b[0;32m      2\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m dqn\u001b[39m.\u001b[39mfit(env, nb_steps\u001b[39m=\u001b[39m\u001b[39m60000\u001b[39m, visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rl\\core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    165\u001b[0m callbacks\u001b[39m.\u001b[39mon_step_begin(episode_step)\n\u001b[0;32m    166\u001b[0m \u001b[39m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(observation)\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_q_values(state)\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[0;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:1320\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(inputs)\n\u001b[0;32m   1319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_predict_function()\n\u001b[1;32m-> 1320\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(inputs)\n\u001b[0;32m   1322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:4555\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4551\u001b[0m         \u001b[39m# We need to do array conversion and type casting at this level,\u001b[39;00m\n\u001b[0;32m   4552\u001b[0m         \u001b[39m# since `callable_fn` only supports exact matches.\u001b[39;00m\n\u001b[0;32m   4553\u001b[0m         tensor_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mas_dtype(tensor\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   4554\u001b[0m         array_vals\u001b[39m.\u001b[39mappend(\n\u001b[1;32m-> 4555\u001b[0m             np\u001b[39m.\u001b[39;49masarray(value, dtype\u001b[39m=\u001b[39;49mtensor_type\u001b[39m.\u001b[39;49mas_numpy_dtype)\n\u001b[0;32m   4556\u001b[0m         )\n\u001b[0;32m   4558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict:\n\u001b[0;32m   4559\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict\u001b[39m.\u001b[39mkeys()):\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0acaf148705ed9ed86cc5cad12259d7985e30670e5686e5f55604a9b3b84a55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
